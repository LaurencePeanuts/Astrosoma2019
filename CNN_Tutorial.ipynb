{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple plotting function which plots 10 exmples along with their predictions\n",
    "def plot(samples,labels):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    gs = gridspec.GridSpec(2, 5)\n",
    "    gs.update(wspace=None, hspace=None)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample, cmap='Greys_r')\n",
    "        number = labels[i].numpy()\n",
    "        plt.title(str(number))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST data using Keras\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that scales 0-255 int images to 0-1 float pixel values\n",
    "def scale(x):\n",
    "    return tf.to_float(x) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct a tensorflow Dataset object with iterators that allows going through all the training and test data easily\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.map(lambda x, y: (scale(x), tf.one_hot(y, 10))).shuffle(10000).batch(30)\n",
    "test_dsD = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_dsD.map(lambda x, y: (scale(x), tf.one_hot(y, 10))).shuffle(10000).batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the model. In the inittialization section we define our layers and in the call we write the forward pass model\n",
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self._input_shape = [-1, 28, 28, 1]\n",
    "        # Declare your layers here. Assign each layer to a variable for this class, which we will use later in \"call\" to process the data.\n",
    "\n",
    "        # use https://www.tensorflow.org/api_docs/python/tf/layers/conv2d with relu (tf.nn.relu) activation\n",
    "        # use filter size of 5 and 32 output channels (number of features) and same padding        \n",
    "        self.conv1 = ...\n",
    "        \n",
    "        # use tf.layers.MaxPooling2D\n",
    "        self.max_pool2d = ...\n",
    "        \n",
    "        # use tf.layers.Conv2D, with filter size of 5 and 64 output channels and relu activation and same padding\n",
    "        self.conv2 = ...\n",
    "\n",
    "        # use tf.layers.Dense with 750 neurons and a relu activation to add a fully connected layer (https://www.tensorflow.org/api_docs/python/tf/layers/dense)\n",
    "        self.fc1 = ...\n",
    "\n",
    "        # add a dropout layer with 0.5 rate: https://www.tensorflow.org/api_docs/python/tf/layers/dropout\n",
    "        self.dropout = ...\n",
    "\n",
    "        # use a tf.layers.Dense layer with 10 neurons (10 digits)\n",
    "        self.fc2 = ...\n",
    "    \n",
    "    def call(self, x):\n",
    "        # First we reshape the input to 1,28,28,1\n",
    "        x = tf.reshape(x, self._input_shape)\n",
    "        # Now the forward pass:\n",
    "        x = self.conv1(x)\n",
    "        # complete the rest of the forward pass below:\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        output = ...\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our loss function\n",
    "def loss_fn(model, x, y):\n",
    "    # predict the logits for your model:\n",
    "    model_logits = model(x)    \n",
    "\n",
    "    # Now use tf.nn.softmax_cross_entropy_with_logits_v2 (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)\n",
    "    # to write a loss given these predictions and the labels.\n",
    "    # use tf.reduce_mean to average over all the batche and all numbers \n",
    "    loss = ...\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An estimate of the accuracy, for the validation and test sets.\n",
    "def get_accuracy(model, x, y_true):\n",
    "    logits = model(x)\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "    equality = tf.equal(prediction, tf.argmax(y_true, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make an instance of our model\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make an instance of an optimizer\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: go through the data, calculate the gradients and apply them to the network weights. As we do this\n",
    "# we also look at the accuracy of the network prediction for the training data at every 10 steps.\n",
    "epochs = 100\n",
    "for (batch, (images, labels)) in enumerate(train_ds):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "    if batch % 10 == 0:\n",
    "        acc = get_accuracy(model, images, labels).numpy()\n",
    "        print(\"Iteration {}, loss: {:.3f}, train accuracy: {:.2f}%\".format(batch, loss_fn(model, images, labels).numpy(), acc*100))\n",
    "    if batch > epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the performance of the network on the test set and plot a few examples\n",
    "avg_acc = 0\n",
    "for (batch, (images, labels)) in enumerate(test_ds):\n",
    "    logits = model(images)\n",
    "    prediction = tf.argmax(logits, 1)\n",
    "    avg_acc += get_accuracy(model, images, labels).numpy()\n",
    "    if batch % 100 == 0 and batch != 0:\n",
    "        plot(images[0:10],prediction[0:10])\n",
    "        print(\"Iteration:{}, Average test accuracy: {:.2f}%\".format(batch, (avg_acc/batch)*100))\n",
    "plot(images[0:10],prediction[0:10])\n",
    "print(\"Final test accuracy: {:.2f}%\".format(avg_acc/batch * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
